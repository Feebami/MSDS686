{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Cats_and_Dogs_Assignment_and_Walkthrough.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxBqGLGQ_kDh"
      },
      "source": [
        "# This tutorial was adapted from Cholett (2021)\n",
        "## You will build a CNN using transfer learning and data augmentation. The data images have already been split into training, validataion, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gml_xoheS5MF"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8nJxNX0S5MX"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from IPython.display import display # Library to help view images\n",
        "from PIL import Image # Library to help view images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Library for data augmentation\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os, shutil # Library for navigating files\n",
        "from google.colab import drive # Library to mount google drives\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR6c1cpY09x2"
      },
      "source": [
        "### We need to mount the google drive to access the images. Paste the authorization code into your browser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMNjfHNhixaa"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmVmwSt1S5Mx"
      },
      "source": [
        "# Specify the base directory where images are located.  You need to access your data here.\n",
        "base_dir = '/content/gdrive/My Drive/Cats_and_Dogs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHMgzJfvS5M8"
      },
      "source": [
        "# Specify the traning, validation, and test dirrectories.  \n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YChCnqH7S5NB"
      },
      "source": [
        "# Specify the the classess in the training, validataion, and test dirrectories\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9TefFy2S5Nk"
      },
      "source": [
        "# Check the number of files in each class dirrectory\n",
        "print(len(os.listdir(train_cats_dir)))\n",
        "print(len(os.listdir(train_dogs_dir)))\n",
        "print(len(os.listdir(validation_cats_dir)))\n",
        "print(len(os.listdir(validation_dogs_dir)))\n",
        "print(len(os.listdir(test_cats_dir)))\n",
        "print(len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD39J_LxS5OL"
      },
      "source": [
        "# We need to normalize the pixels in the images.  The data will 'flow' through this generator.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQd1QktFS5OP"
      },
      "source": [
        "# Since the file images are in a directory we need to move them from the directory into the model.  \n",
        "# Keras as a function that makes this easy. Documentaion is here: https://keras.io/preprocessing/image/\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, # The directory where the train data is located\n",
        "    target_size=(150, 150), # Reshape the image to 150 by 150 pixels. This is important because it makes sure all images are the same size.\n",
        "    batch_size=20, # We will take images in batches of 20.\n",
        "    class_mode='binary') # The classification is binary.\n",
        "\n",
        "validataion_generator = train_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a plotting function\n",
        "def plot_history():\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  acc_values = history_dict['accuracy']\n",
        "  val_acc_values = history_dict['val_accuracy']\n",
        "  epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "  plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
        "  plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  return plt.show()"
      ],
      "metadata": {
        "id": "oqNR8TyYLaeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model\n",
        "def Base_CNN():\n",
        "  backend.clear_session()\n",
        "  model = models.Sequential()\n",
        "\n",
        "  model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (150, 150, 3)))\n",
        "  model.add(layers.MaxPool2D((2,2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(layers.Conv2D(32, (3,3), activation = 'relu'))\n",
        "  model.add(layers.MaxPool2D((2,2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(layers.Conv2D(32, (3,3), activation = 'relu'))\n",
        "  model.add(layers.MaxPool2D((2,2)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer = 'rmsprop',\n",
        "                loss = 'binary_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "aLdrG2n1L2oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Base_CNN()"
      ],
      "metadata": {
        "id": "bdBnSmU1HoPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1oqUEMpvT2H"
      },
      "source": [
        "history = model.fit( # The image data must come from the image generator that takes the images from the correct dirrectory. https://keras.io/models/sequential/\n",
        "    train_generator, # Images are taken from the train_generator\n",
        "    steps_per_epoch=100, # The number of steps from the train_generator before one epoch if finished.  \n",
        "                         # 100 steps * 20 batch size in train generator = 2000 training images per epoch. This way each traning image will be sampled once per epoch.\n",
        "    epochs=50, # Train data for 50 epochs\n",
        "    validation_data=validataion_generator, # Take data from the validataion generator\n",
        "    validation_steps=50, # 50 steps * 20 batch size in validation generator = 1000 validation images per epoch\n",
        "    verbose = 1,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)]) # We will not use call backs to stop early.\n",
        "\n",
        "plot_history() # Use our plot function to plot the loss and accuracy.\n",
        "\n",
        "test_loss, test_acc =model.evaluate(test_generator, steps = 50) # Test images are in a dirrectory so they must flow from dirrectory. \n",
        "                                                                           # 50 steps * 20 batch size in test generator = 1000 test images per epoch\n",
        "print('test_acc:', test_acc)\n",
        "\n",
        "model.save('cats_and_dogs_base.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vRcaO1dKItm"
      },
      "source": [
        "### The above model came out with about 70% accuracy with severe overfitting. Which is pretty good considering we only used 2000 training images!  Now lets improve using data augmentation.\n",
        "#### Data augmentation allows us to randomally transform images before sending them to the model for training.  The random transformation changes the images into 'new' images and allows for an increase in traning data without have additional images. https://keras.io/preprocessing/image/ \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lE-v1s-S5Og"
      },
      "source": [
        "datagen = ImageDataGenerator( # The image augmentaion function in Keras\n",
        "    rotation_range=40, # Rotate the images randomly by 40 degrees\n",
        "    width_shift_range=0.2, # Shift the image horizontally by 20%\n",
        "    height_shift_range=0.2, # Shift the image veritcally by 20%\n",
        "    shear_range=0.2, # Shear image by 20%\n",
        "    zoom_range=0.2, # Zoom in on image by 20%\n",
        "    horizontal_flip=True, # Flip image horizontally \n",
        "    fill_mode='nearest') # How to fill missing pixels after a augmentaion opperation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6n24_ZUS5Oi"
      },
      "source": [
        "# Lets see the image augmentaion\n",
        "from tensorflow.keras.preprocessing import image\n",
        "img = image.load_img(os.path.join(train_cats_dir, os.listdir(train_cats_dir)[1]), target_size=(150,150))\n",
        "x = image.img_to_array(img)\n",
        "x = x.reshape((1,) + x.shape)\n",
        "i = 0 \n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    plt.figure(i)\n",
        "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "    i += 1\n",
        "    if i % 4 == 0:\n",
        "        break\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXgxrxmjS5Os"
      },
      "source": [
        "# Apply the data augmentation to our data.\n",
        "train_datagen2 = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen2 = ImageDataGenerator(rescale=1./255) #Never apply data augmentation to test data.  You only want to normalize and resize test data. \n",
        "\n",
        "train_generator2 = train_datagen2.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validataion_generator2 = train_datagen2.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator2 = test_datagen2.flow_from_directory( # Resize test data\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFvmfqmS5On"
      },
      "source": [
        "history = model.fit(\n",
        "    train_generator2,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=50,\n",
        "    validation_data=validataion_generator2,\n",
        "    validation_steps=50,\n",
        "    verbose = 1,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
        "\n",
        "\n",
        "plot_history()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator2, steps = 50)\n",
        "print('test_acc:', test_acc)\n",
        "\n",
        "model.save('cats_and_dogs_base_augment.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1jeroqqObPW"
      },
      "source": [
        "## An inprovment, but not a suprise. Having more data helps our accuracy. \n",
        "### But why go through the hassle of building our own CNN when there are many networks that have used powerful GPUs to classify images? We can use the weights of their models and apply them to something as simple as classiying a dog and cat.  We will use weights of the VGG16 CNN that was trained using ImageNet data.  https://keras.io/applications/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xidIScoOS5O0"
      },
      "source": [
        "from keras.applications import VGG16 # Import the VGG16 library. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nbeVW1NS5O1"
      },
      "source": [
        "backend.clear_session()\n",
        "conv_base = VGG16 (weights = 'imagenet', #Useing the VGG66 CNN that was trained on ImageNet data.  \n",
        "                  include_top = False, # We are using our own classification (dog or cat) and not the ImageNet multiclassification. So include top = false.\n",
        "                  input_shape = (150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e746eXq5S5O4"
      },
      "source": [
        "conv_base.summary() # View the VGG16 model architecture."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm1-txlUQmJ_"
      },
      "source": [
        "### The output shape of the data after the VGG16 weights are applied is (4, 4, 512).  Remember it started as (150, 150, 3).  This is from the 2D convolutions and maxpooling steps.  There are also over 14 million parameters to train.  This is way too many for the Google GPU. And we do not want to retrain the weights of the VGG16, that defeats the whole purpose of transfer learning.  Instead we will freeze the VGG16 weights and add a dense layer at the end.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqLz1oGSWckI"
      },
      "source": [
        "conv_base.trainable = False # Freeze the VGG16 weights."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(conv_base)"
      ],
      "metadata": {
        "id": "W8LcggkAsp2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KBREKXYWlk2"
      },
      "source": [
        "conv_base.summary()# Lets look at the parameters now."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3O0iAXbWs3p"
      },
      "source": [
        "### Now the trainable parameters are 0.  We will only use the pretraind VGG16 weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdXiFRx6S5Pa"
      },
      "source": [
        "def modelvgg():\n",
        "  backend.clear_session()\n",
        "  modelvgg16 = models.Sequential()\n",
        "  modelvgg16.add(conv_base) # Add the VG166 weights\n",
        "  modelvgg16.add(layers.Flatten())\n",
        "  modelvgg16.add(layers.Dense(512, activation = 'relu'))\n",
        "  modelvgg16.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  modelvgg16.compile(optimizer = 'rmsprop',\n",
        "                      loss = 'binary_crossentropy',\n",
        "                      metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "  return modelvgg16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = modelvgg()"
      ],
      "metadata": {
        "id": "MwETvQtqH2le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model)"
      ],
      "metadata": {
        "id": "sI9hHNSZlk6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijIeC7J1S5Pn"
      },
      "source": [
        "# We will still use data augmentation\n",
        "history = model.fit(\n",
        "    train_generator2,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=50,\n",
        "    validation_data=validataion_generator2,\n",
        "    validation_steps=50,\n",
        "    verbose = 1,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
        "\n",
        "plot_history()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator2, steps = 50)\n",
        "print('test_acc:', test_acc)\n",
        "\n",
        "model.save('cats_and_dogs_vgg.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EccdeBlhXq_K"
      },
      "source": [
        "### Our model keeps getting better. We might want to add a few 2D convolutions and max pooling layers before the dense layer, or we can train the last three 2D convolution layers and maxpooling layer of the VGG16 model. We must be stratigic when choosing which layers to train in the transfer model.  We want to train convolution, max pooling, batch normalization, etc., layers that are part of block.  It would not make sense to only train max pooling or batch normalization layers without also training the convolution layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGXPJvzoS5Pq"
      },
      "source": [
        "# Now we can freeze all the VGG weights except the last few, and train those before adding it to our dense layer.\n",
        "backend.clear_session()\n",
        "vgg16_base_2 = VGG16(weights = 'imagenet', include_top = False, input_shape = (150, 150, 3))\n",
        "\n",
        "# Here we freeze all the layers except the last 2.\n",
        "for layer in vgg16_base_2.layers[:-2]:\n",
        "  layer.trainable = False\n",
        "for layer in vgg16_base_2.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHl7lEDFC4Yu"
      },
      "source": [
        "vgg16_base_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(vgg16_base_2)"
      ],
      "metadata": {
        "id": "90mnL5tks0Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_vgg_train():\n",
        "  backend.clear_session()\n",
        "  modelvgg16_train = models.Sequential()\n",
        "  modelvgg16_train.add(vgg16_base_2)\n",
        "  modelvgg16_train.add(layers.Flatten())\n",
        "  modelvgg16_train.add(layers.Dense(512, activation = 'relu'))\n",
        "  modelvgg16_train.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  modelvgg16_train.compile('rmsprop',\n",
        "    loss = 'binary_crossentropy',\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "  return modelvgg16_train"
      ],
      "metadata": {
        "id": "mqh1znPCpea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_vgg_train()"
      ],
      "metadata": {
        "id": "rYuZcPb6IA-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOQLiSzZCWHY"
      },
      "source": [
        "history = model.fit(\n",
        "    train_generator2,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=50,\n",
        "    validation_data=validataion_generator2,\n",
        "    validation_steps=50,\n",
        "    verbose = 1,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights = True)])\n",
        "\n",
        "\n",
        "plot_history()\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator2, steps = 50)\n",
        "print('test_acc:', test_acc)\n",
        "\n",
        "model.save('cats_and_dogs_small_vgg_train.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The best model yet!"
      ],
      "metadata": {
        "id": "U7uaQvzIw7_d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsuF9mMLpDyY"
      },
      "source": [
        "# Your Turn\n",
        "### Build and optimize another model. Use weights from a different pretrained network (ie, ResNet, Inception, etc. not VGG. https://keras.io/applications/) from the Keras library. Optimize the model by adding additional layers, regularization, change activaction, adjust data augmentation etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sri5vWATxg5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}